{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a165b09b",
   "metadata": {},
   "source": [
    "# Final Project: Face Mask Detection\n",
    "Welcome to the final project assignment. For this project, we will be working on the Face Mask detection problem.\n",
    "\n",
    "The Dataset contains images of people wearing masks and people not wearing masks. The database contains 10,000 colored images in the training folder, 800 images in the validation folder, and 992 images in the test folder.\n",
    "\n",
    "Your task is to create a CNN model for identifying whether a person in the image is wearing a mask or not.\n",
    "\n",
    "Please perform the following tasks:\n",
    "\n",
    "Download the database, click here to download it.\n",
    "Create test, train, and validation directory variables\n",
    "Create train and validation data generator with target size (128,128)\n",
    "Train a CNN model\n",
    "Train a model with VGG19 model\n",
    "Use callbacks to save your model at every step\n",
    "Training may take several hours, so use 5 to 10 epochs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e610e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91b6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd2f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two folders are present inside each full path directory which contain the images\n",
    "train_dir = r'C:\\Users\\Deeshu\\Desktop\\Face Mask Detection Using Deep Learning\\Datasets\\Train'\n",
    "validation_dir =r'C:\\Users\\Deeshu\\Desktop\\Face Mask Detection Using Deep Learning\\Datasets\\Validation'\n",
    "test_dir =r'C:\\Users\\Deeshu\\Desktop\\Face Mask Detection Using Deep Learning\\Datasets\\Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77d51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd180b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2146f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the VGG19 to use its Conv Base\n",
    "from tensorflow.keras.applications import VGG19\n",
    "# weights =  we are using the final weights (pre trained weights)\n",
    "# imagenet is ILSVRC Competition so its weights\n",
    "# include top = false, because we are only using the conv base\n",
    "# If wanted to import the whole model then include_top=True\n",
    "# input shape = normal input shape of our images {target size + channels}\n",
    "conv_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5584362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721b8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b0271f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 4, 4, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2097408   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,122,049\n",
      "Trainable params: 22,122,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae2df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "# using small learning rate becoz we want to fine tune our model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0237ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"Face_Mask_Detect_Model-{epoch:02d}.h5\",save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfbfc3f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1796 - acc: 0.9245WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 872s 9s/step - loss: 0.1796 - acc: 0.9245 - val_loss: 0.0244 - val_acc: 0.9925\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0589 - acc: 0.9800 WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 1122s 11s/step - loss: 0.0589 - acc: 0.9800\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0534 - acc: 0.9865WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 513s 5s/step - loss: 0.0534 - acc: 0.9865\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0309 - acc: 0.9895WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 430s 4s/step - loss: 0.0309 - acc: 0.9895\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0300 - acc: 0.9900WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 436s 4s/step - loss: 0.0300 - acc: 0.9900\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0309 - acc: 0.9910WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 457s 5s/step - loss: 0.0309 - acc: 0.9910\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0368 - acc: 0.9875WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 450s 5s/step - loss: 0.0368 - acc: 0.9875\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0279 - acc: 0.9920WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 498s 5s/step - loss: 0.0279 - acc: 0.9920\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0224 - acc: 0.9920WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 481s 5s/step - loss: 0.0224 - acc: 0.9920\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0113 - acc: 0.9960WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "100/100 [==============================] - 440s 4s/step - loss: 0.0113 - acc: 0.9960\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "213633d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.1795806884765625,\n",
       "  0.05893730744719505,\n",
       "  0.053403038531541824,\n",
       "  0.030874839052557945,\n",
       "  0.029992982745170593,\n",
       "  0.030900919809937477,\n",
       "  0.03677445650100708,\n",
       "  0.027941972017288208,\n",
       "  0.02235162816941738,\n",
       "  0.011336948722600937],\n",
       " 'acc': [0.9244999885559082,\n",
       "  0.9800000190734863,\n",
       "  0.9865000247955322,\n",
       "  0.9894999861717224,\n",
       "  0.9900000095367432,\n",
       "  0.9909999966621399,\n",
       "  0.987500011920929,\n",
       "  0.9919999837875366,\n",
       "  0.9919999837875366,\n",
       "  0.9959999918937683],\n",
       " 'val_loss': [0.024365201592445374],\n",
       " 'val_acc': [0.9925000071525574]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82b3bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 992 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d1e4c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 43s 855ms/step - loss: 0.0134 - acc: 0.9960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.013427396304905415, 0.9959677457809448]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#99.5% Accuracy Achieved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb726e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507958d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcd78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770a403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847ccf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a1c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf4b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d1fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1be020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747998e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a472e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1991d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f851656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc83f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d2e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
